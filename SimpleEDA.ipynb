{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.graphics.correlation import plot_corr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "# df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Print the first 10 rows of the data\n",
    "print(df.head(10))\n",
    "\n",
    "# Get a summary of the data\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values before handling:\\n\", df.isnull().sum())\n",
    "\n",
    "# Check the number of unique values\n",
    "print(\"Number of unique values:\\n\", df.nunique())\n",
    "\n",
    "# Check the value counts for each column\n",
    "for col in df.columns:\n",
    "    print(f'Value counts for {col}:\\n', df[col].value_counts())\n",
    "\n",
    "\n",
    "# Handle missing values by filling with mean\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Check for missing values again\n",
    "print(\"Missing values after handling:\\n\", df.isnull().sum())\n",
    "\n",
    "# Check the data types\n",
    "print(\"Data types:\\n\", df.dtypes)\n",
    "\n",
    "# If any categorical variables, convert them into dummy variables\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df = pd.concat(\n",
    "            [df.drop(col, axis=1), pd.get_dummies(df[col], prefix=col)], axis=1)\n",
    "\n",
    "# Check the data types again\n",
    "print(\"Data types after handling categorical variables:\\n\", df.dtypes)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Check the correlation matrix and visualize it\n",
    "corr = df_scaled.corr()\n",
    "print(\"Correlation matrix:\\n\", corr)\n",
    "\n",
    "# Plotting a heatmap using seaborn\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(corr, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Print Skewness and Kurtosis\n",
    "for col in df.columns:\n",
    "    print(f'Skewness of {col}: {df[col].skew()}')\n",
    "    print(f'Kurtosis of {col}: {df[col].kurt()}')\n",
    "\n",
    "# Histograms\n",
    "for col in df.columns:\n",
    "    plt.figure()\n",
    "    plt.hist(df[col], bins=50)\n",
    "    plt.title(f'Histogram of {col}')\n",
    "    plt.show()\n",
    "\n",
    "# Boxplots\n",
    "for col in df.columns:\n",
    "    plt.figure()\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.show()\n",
    "\n",
    "# Scatterplots\n",
    "for i in range(len(df.columns)):\n",
    "    for j in range(i+1, len(df.columns)):\n",
    "        plt.figure()\n",
    "        plt.scatter(df[df.columns[i]], df[df.columns[j]])\n",
    "        plt.title(f'Scatterplot of {df.columns[j]} vs {df.columns[i]}')\n",
    "        plt.xlabel(df.columns[i])\n",
    "        plt.ylabel(df.columns[j])\n",
    "        plt.show()\n",
    "\n",
    "# T-tests\n",
    "for col in df.columns:\n",
    "    t_stat, p_val = stats.ttest_1samp(df[col], 0)\n",
    "    print(\n",
    "        f'T-test for {col}: t statistic = {t_stat:.3f}, p-value = {p_val:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "# Data Imputation: Replacing missing values using SimpleImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# One-Hot Encoding for Categorical variables\n",
    "df_encoded = pd.get_dummies(df_imputed, drop_first=True)\n",
    "\n",
    "# Pairwise correlations of different categories.\n",
    "print(\"Pairwise correlation of different categories:\")\n",
    "pairwise_corr = df_encoded.corr().abs()\n",
    "print(pairwise_corr)\n",
    "\n",
    "# Outlier detection using Z-score\n",
    "z_scores = np.abs(stats.zscore(df_encoded))\n",
    "df_outliers_removed = df_encoded[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# Multiple Scatter Plots using seaborn's pairplot\n",
    "sns.pairplot(df_outliers_removed)\n",
    "plt.show()\n",
    "\n",
    "# Chi-Square Test for Categorical variables\n",
    "chi2_stat, p_val, dof, ex = stats.chi2_contingency(df_encoded)\n",
    "print(f'Chi-Square Statistic : {chi2_stat}\\n')\n",
    "print(f'p-value : {p_val}\\n')\n",
    "\n",
    "# Linear Regression model fitting and summary using statsmodels\n",
    "X = sm.add_constant(df_encoded.iloc[:, :-1])  # adding a constant\n",
    "Y = df_encoded.iloc[:, -1]\n",
    "model = sm.OLS(Y, X).fit()\n",
    "predictions = model.predict(X)\n",
    "print(model.summary())\n",
    "\n",
    "# PCA for dimensionality reduction and visualization\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(df_encoded)\n",
    "principalDf = pd.DataFrame(data=principalComponents, columns=['PC1', 'PC2'])\n",
    "sns.scatterplot(data=principalDf, x='PC1', y='PC2')\n",
    "plt.show()\n",
    "\n",
    "# Calculating VIF to check for multicollinearity\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(\n",
    "    X.values, i) for i in range(len(X.columns))]\n",
    "print(vif_data)\n",
    "\n",
    "# Plotting distribution of each variable\n",
    "for col in df_encoded.columns:\n",
    "    sns.distplot(df_encoded[col])\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.show()\n",
    "\n",
    "# Grouping and aggregating data\n",
    "grouped_data = df_encoded.groupby(df_encoded.columns[0]).mean()\n",
    "print(\"Grouped and aggregated data:\\n\", grouped_data)\n",
    "\n",
    "# Calculating pairwise differences between each pair of columns\n",
    "for i in range(len(df_encoded.columns)):\n",
    "    for j in range(i + 1, len(df_encoded.columns)):\n",
    "        diff = df_encoded[df_encoded.columns[i]] - \\\n",
    "            df_encoded[df_encoded.columns[j]]\n",
    "        print(\n",
    "            f'Difference between {df_encoded.columns[i]} and {df_encoded.columns[j]}: {diff}')\n",
    "\n",
    "# Bivariate Analysis\n",
    "for i in range(len(df_encoded.columns)):\n",
    "    for j in range(i + 1, len(df_encoded.columns)):\n",
    "        sns.jointplot(x=df_encoded[df_encoded.columns[i]],\n",
    "                      y=df_encoded[df_encoded.columns[j]])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume the last column is the target variable and is categorical\n",
    "X = df_encoded.iloc[:, :-1]\n",
    "y = df_encoded.iloc[:, -1]\n",
    "\n",
    "# Convert target variable to categorical (one-hot encoding)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)\n",
    "dummy_y = np_utils.to_categorical(encoded_y)\n",
    "\n",
    "# Split the dataset into the training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, dummy_y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define a neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "# Multi-class classification\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, validation_data=(\n",
    "    X_test, y_test), epochs=50, batch_size=10)\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4\n",
    "import seaborn as sns\n",
    "\n",
    "# Let's consider df_encoded for this visualisation\n",
    "df_vis = df_encoded.copy()\n",
    "\n",
    "# 1. Heatmap for correlation\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_vis.corr(), cmap='coolwarm')\n",
    "plt.title(\"Heatmap of Correlation\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Pairplot - plots pairwise relationships in a dataset\n",
    "sns.pairplot(df_vis.sample(n=100))  # Using a sample for efficiency\n",
    "plt.title(\"Pairplot of Variables\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Histogram for each column\n",
    "for col in df_vis.columns:\n",
    "    sns.histplot(data=df_vis, x=col, kde=True)\n",
    "    plt.title(f'Histogram of {col}')\n",
    "    plt.show()\n",
    "\n",
    "# 4. Boxplot for each column\n",
    "for col in df_vis.columns:\n",
    "    sns.boxplot(x=df_vis[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.show()\n",
    "\n",
    "# 5. Violinplot for each column\n",
    "for col in df_vis.columns:\n",
    "    sns.violinplot(x=df_vis[col])\n",
    "    plt.title(f'Violin Plot of {col}')\n",
    "    plt.show()\n",
    "\n",
    "# 6. Scatter plot for first two columns\n",
    "sns.scatterplot(data=df_vis, x=df_vis.columns[0], y=df_vis.columns[1])\n",
    "plt.title(\"Scatterplot\")\n",
    "plt.show()\n",
    "\n",
    "# 7. Countplot for first column\n",
    "sns.countplot(x=df_vis[df_vis.columns[0]])\n",
    "plt.title(\"Count Plot\")\n",
    "plt.show()\n",
    "\n",
    "# 8. Barplot - For this, let's consider first two columns\n",
    "sns.barplot(x=df_vis.columns[0], y=df_vis.columns[1], data=df_vis)\n",
    "plt.title(\"Bar Plot\")\n",
    "plt.show()\n",
    "\n",
    "# 9. Distribution plot for first column\n",
    "sns.displot(df_vis, x=df_vis.columns[0], kde=True)\n",
    "plt.title(\"Distribution Plot\")\n",
    "plt.show()\n",
    "\n",
    "# 10. Joint plot for first two columns\n",
    "sns.jointplot(data=df_vis, x=df_vis.columns[0], y=df_vis.columns[1])\n",
    "plt.title(\"Joint Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Let's assume we have two dataframes: df1 and df2\n",
    "\n",
    "# Load your data\n",
    "df1 = pd.read_csv('dataset1.csv')\n",
    "df2 = pd.read_csv('dataset2.csv')\n",
    "\n",
    "# Let's say we're interested in comparing a particular column from both dataframes\n",
    "col1 = df1['YourColumn']\n",
    "col2 = df2['YourColumn']\n",
    "\n",
    "# Perform ANOVA\n",
    "f_stat, p_val = stats.f_oneway(col1, col2)\n",
    "\n",
    "print(\"F-statistic:\", f_stat)\n",
    "print(\"P-value:\", p_val)\n",
    "\n",
    "# Interpretation:\n",
    "if p_val < 0.05:\n",
    "    print(\"There is a significant difference between the means of the two datasets.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the means of the two datasets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform T-test\n",
    "t_stat, p_val_t = stats.ttest_ind(col1, col2)\n",
    "\n",
    "print(\"T-statistic:\", t_stat)\n",
    "print(\"P-value:\", p_val_t)\n",
    "\n",
    "# Interpretation\n",
    "if p_val_t < 0.05:\n",
    "    print(\"There is a significant difference between the means of the two datasets (T-test).\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the means of the two datasets (T-test).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume that the variables 'categorical_var1' and 'categorical_var2' are categorical variables\n",
    "\n",
    "# Perform Chi-square test\n",
    "chi_stat, p_val_chi, dof, ex = stats.chi2_contingency(\n",
    "    pd.crosstab(df1['categorical_var1'], df2['categorical_var2']))\n",
    "\n",
    "print(\"Chi-square statistic:\", chi_stat)\n",
    "print(\"P-value:\", p_val_chi)\n",
    "\n",
    "# Interpretation\n",
    "if p_val_chi < 0.05:\n",
    "    print(\"There is a significant relationship between the two categorical variables.\")\n",
    "else:\n",
    "    print(\"There is no significant relationship between the two categorical variables.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Mann-Whitney U Test\n",
    "u_stat, p_val_u = stats.mannwhitneyu(col1, col2)\n",
    "\n",
    "print(\"U statistic:\", u_stat)\n",
    "print(\"P-value:\", p_val_u)\n",
    "\n",
    "# Interpretation\n",
    "if p_val_u < 0.05:\n",
    "    print(\"There is a significant difference between the distributions of the two datasets.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the distributions of the two datasets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.multivariate.manova import MANOVA\n",
    "# Assuming that df1 and df2 are combined into one dataframe and group labels are stored in df['group']\n",
    "manova = MANOVA.from_formula('col1 + col2 + col3 ~ group', data=df)\n",
    "print(manova.mv_test())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Kruskal-Wallis H-test: This is the non-parametric version of ANOVA, used when the assumptions of ANOVA are not met (e.g., when the dependent variable is not normally distributed).\n",
    "\"\"\"\n",
    "\n",
    "from scipy.stats import kruskal\n",
    "stat, p = kruskal(df1[\"column\"], df2[\"column\"])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Two-way ANOVA: You would use two-way ANOVA when you want to know how two independent variables, in combination, affect a dependent variable.\n",
    "\"\"\"\n",
    "\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "# Assuming that df1 and df2 are combined into one dataframe, with a \"treatment\" column indicating the group\n",
    "res = AnovaRM(df, 'dependent_var', 'subject_id', within=[\n",
    "              'independent_var1', 'independent_var2'], aggregate_func='mean').fit()\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generalized Linear Models (GLM): These models extend the ordinary linear regression models to allow for response variables that have error distribution models other than a normal distribution.\n",
    "\"\"\"\n",
    "\n",
    "import statsmodels.api as sm\n",
    "glm_binom = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "res = glm_binom.fit()\n",
    "print(res.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Time Series Analysis (TSA): If you are dealing with time-series data, TSA tests can be helpful. This includes autocorrelation, partial autocorrelation, and Dickey-Fuller tests among others.\n",
    "\"\"\"\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "result = adfuller(df['column'])\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Survival Analysis: This set of statistical approaches is used to analyze the time until the occurrence of an event. Libraries like lifelines in Python provide functionalities for this.\n",
    "\"\"\"\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(T, E)\n",
    "kmf.plot_survival_function()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_ind, f_oneway, kruskal\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# T-Test\n",
    "# This test is used to determine if the means of two sets of data (arr1 and arr2) are significantly different from each other.\n",
    "# arr1, arr2: Arrays (or lists) containing the numerical data of two independent groups\n",
    "# Output: t_statistic (the calculated t-statistic), p_value (two-tailed p-value)\n",
    "t_statistic, p_value = ttest_ind(arr1, arr2)\n",
    "print(f\"T-test:\\nT-statistic: {t_statistic}, P-value: {p_value}\\n\")\n",
    "\n",
    "# One-way ANOVA\n",
    "# This test is used to determine if there are any statistically significant differences between the means of three or more independent groups (arr1, arr2, and arr3).\n",
    "# arr1, arr2, arr3: Arrays (or lists) containing the numerical data of three or more independent groups\n",
    "# Output: f_statistic (the calculated f-statistic), p_value (two-tailed p-value)\n",
    "f_statistic, p_value = f_oneway(arr1, arr2, arr3)\n",
    "print(f\"ANOVA:\\nF-statistic: {f_statistic}, P-value: {p_value}\\n\")\n",
    "\n",
    "# MANOVA\n",
    "# This test extends ANOVA for cases where there are two or more dependent variables (num1 and num2), it is used to test if category (cat) has different effects on the dependent variables.\n",
    "# df: DataFrame containing the data\n",
    "# num1, num2: Column names of the numerical dependent variables\n",
    "# cat: Column name of the categorical independent variable\n",
    "# Output: Pillai's trace, Wilks' lambda, Hotelling-Lawley trace, and Roy's greatest root statistics and associated p-values\n",
    "mv = MANOVA.from_formula('num1 + num2 ~ cat', data=df)\n",
    "print(f\"MANOVA:\\n{mv.mv_test()}\\n\")\n",
    "\n",
    "# Kruskal-Wallis H-test\n",
    "# This test is used to determine if there are statistically significant differences between two or more groups of an independent variable on a continuous or ordinal dependent variable (arr1, arr2, and arr3).\n",
    "# arr1, arr2, arr3: Arrays (or lists) containing the numerical data of three or more independent groups\n",
    "# Output: h_statistic (the calculated Kruskal-Wallis H statistic), p_value (two-tailed p-value)\n",
    "h_statistic, p_value = kruskal(arr1, arr2, arr3)\n",
    "print(\n",
    "    f\"Kruskal-Wallis H-test:\\nH-statistic: {h_statistic}, P-value: {p_value}\\n\")\n",
    "\n",
    "# Two-way ANOVA\n",
    "# This test is used to determine how two different categorical variables (cat1 and cat2) interact to affect a dependent variable (num).\n",
    "# df: DataFrame containing the data\n",
    "# num: Column name of the numerical dependent variable\n",
    "# cat1, cat2: Column names of the categorical independent variables\n",
    "# Output: Summary table containing sum of squares, mean square, f-statistic, and p-value for each factor and interaction\n",
    "model = ols('num ~ C(cat1) + C(cat2) + C(cat1):C(cat2)', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "print(f\"Two-way ANOVA:\\n{anova_table}\\n\")\n",
    "\n",
    "# Generalized Linear Models (GLM)\n",
    "# GLM is a flexible generalization of ordinary linear regression that allows for response variables that have other than a normal distribution, it relates a function of the expected response variable y to a linear combination of predictors X.\n",
    "# y: Array (or list) containing the numerical dependent variable\n",
    "# X: 2D array (or DataFrame) containing the numerical independent variables\n",
    "# Output: Summary table containing coefficients, standard errors, z-statistic, and p-value for each predictor\n",
    "model = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "result = model.fit()\n",
    "print(f\"GLM:\\n{result.summary()}\\n\")\n",
    "\n",
    "# Time Series Analysis (TSA)\n",
    "# The Augmented Dickey–Fuller (ADF) test is a type of statistical test called a unit root test, it tests if the time series (ts) is stationary (mean-reverting) or not.\n",
    "# ts: Array (or list) containing the time series data\n",
    "# Output: ADF statistic, p-value, used lag, number of observations, critical values for the ADF statistic, and the maximized information criterion\n",
    "result = adfuller(ts)\n",
    "print(f\"ADF test:\\nADF Statistic: {result[0]}, P-value: {result[1]}\\n\")\n",
    "\n",
    "# Survival Analysis\n",
    "# Kaplan–Meier estimator, also known as the product limit estimator, is a non-parametric statistic used to estimate the survival function from lifetime data (T and E), it is used to measure the fraction of subjects living for a certain amount of time after treatment.\n",
    "# T: Array (or list) containing the time-to-event\n",
    "# E: Array (or list) containing the event indicator (1=event, 0=censor)\n",
    "# Output: Kaplan-Meier survival function plot\n",
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(T, event_observed=E)\n",
    "kmf.plot_survival_function()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
